{
 "cells": [
  {
   "cell_type": "raw",
   "id": "966ec770",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d4e25fae",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "# 1. Download and load the model\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "#model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "f53b992f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "# For Llama models, you'll need to use AutoModelForCausalLM instead of AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53eac6a0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "# 2. Save locally\n",
    "local_directory = \"./llama_3_2_3b_instruct\"\n",
    "\n",
    "model.save_pretrained(local_directory)\n",
    "tokenizer.save_pretrained(local_directory)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5156569",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "# 3. Load from local directory later\n",
    "local_model = AutoModelForCausalLM.from_pretrained(local_directory)\n",
    "local_tokenizer = AutoTokenizer.from_pretrained(local_directory)\n",
    "\n",
    "# 4. Generate text with the model\n",
    "inputs = tokenizer(\"Write a short poem about programming:\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90a8ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51ada4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cbcfbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb886a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Download and load the model with Mac-compatible quantization\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "#model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Authentication required\n",
    "hf_token = \"-\"  # Replace with your actual HuggingFace token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a9e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6c78fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For Mac, use MPS (Metal Performance Shaders) if available or float16 precision\n",
    "import torch\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e984dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load model with reduced precision\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    token=hf_token,\n",
    "    torch_dtype=torch.float16,  # Use half precision\n",
    "    device_map=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df2fdc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_directory = \"./llama_3_2_3b_instruct_mac\"\n",
    "#local_directory = \"./tiny_llama_1_1b_chat_v1_0_mac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847469b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Save locally\n",
    "model.save_pretrained(local_directory)\n",
    "tokenizer.save_pretrained(local_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17ea9e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b9cb9127d94250915cbae8f8b910a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 3. Load from local directory later\n",
    "local_model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_directory,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device\n",
    ")\n",
    "local_tokenizer = AutoTokenizer.from_pretrained(local_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e6404c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "274b7366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=local_model,\n",
    "    tokenizer=local_tokenizer,\n",
    "    device_map=device,\n",
    "    #device=device,\n",
    "    max_length=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    top_k=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31c7576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt = \"What is the capital of Italy?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cca7edd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Very easy to work with them!\n",
    "\n",
    "Fast, simple and clear process. A little expensive if you keep the loan for the entire period but it is up to the borrower whether you repay earlier or not.\"\"\"\n",
    "\n",
    "\n",
    "text = \"\"\"Bad idea not to announce interest rate cuts for the savings account.\n",
    "First of all, I just want to mention that it is not possible to log in to their site if you have a VPN running. I tried for many days (a bit rough when you can't get into your savings account) before I thought of trying to turn off the VPN.\n",
    "Quite a high interest rate on the savings account but, as many others have mentioned, you don't get any notification whatsoever about interest rate cuts. This sets them apart from all the other banks I have an account with, where you get text messages, emails or at least a notification to your account. At Qred, you have to check yourself, at regular intervals, that they haven't lowered the interest rate.\n",
    "It should be easy for them to fix, so I strongly suspect that they simply don't want to advertise interest rate cuts, which brings my rating down to a weak third.\"\"\"\n",
    "\n",
    "\n",
    "text = \"\"\"Rude customer service with a terrible attitude.\n",
    "\n",
    "Indicates that the submitted power of attorney is not approved. The power of attorney is approved by all district courts in Sweden. Sweden's worst treatment.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5318b0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Consider the following review of a loan company. What is the sentiment of the review? Just respond with Positive, Negative or Neutral; do no add anything else.\n",
    "{text}\n",
    "Sentiment:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "966542fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider the following review of a loan company. What is the sentiment of the review? Just respond with Positive, Negative or Neutral; do no add anything else.\n",
      "Rude customer service with a terrible attitude.\n",
      "\n",
      "Indicates that the submitted power of attorney is not approved. The power of attorney is approved by all district courts in Sweden. Sweden's worst treatment.\n",
      "Sentiment: Negative.\n"
     ]
    }
   ],
   "source": [
    "#response = text_generator(\"who are you?\")#, max_new_tokens=50)\n",
    "response = text_generator(prompt)\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61552f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a397727f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Generate text with the model\n",
    "prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "Write a short poem about programming.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "inputs = local_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = local_model.generate(\n",
    "    **inputs, \n",
    "    max_length=200, \n",
    "    temperature=0.7, \n",
    "    top_p=0.9\n",
    ")\n",
    "print(local_tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0039b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d785b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1642e3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bc8f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc28c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af05f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f81012",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Set up quantization configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c65d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Download and load the model with quantization\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# Authentication required\n",
    "hf_token = \"-\"  # Replace with your actual HuggingFace token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9be2a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    token=hf_token,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"  # Automatically decide device placement\n",
    ")\n",
    "\n",
    "# 3. Save locally\n",
    "local_directory = \"./llama_3_2_3b_instruct_quantized\"\n",
    "model.save_pretrained(local_directory)\n",
    "tokenizer.save_pretrained(local_directory)\n",
    "\n",
    "# 4. Load from local directory later\n",
    "# Note: When loading quantized models, you need to specify the quantization config again\n",
    "local_model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_directory,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "local_tokenizer = AutoTokenizer.from_pretrained(local_directory)\n",
    "\n",
    "# 5. Generate text with the model\n",
    "prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "Write a short poem about programming.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(local_model.device)\n",
    "outputs = local_model.generate(**inputs, max_length=200, temperature=0.7, top_p=0.9)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1d2fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
